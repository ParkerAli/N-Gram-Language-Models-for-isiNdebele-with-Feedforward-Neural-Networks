{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"trigram_baseline.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"cells":[{"cell_type":"code","metadata":{"id":"UwDH8EIaXGk4"},"source":["!nvidia-smi # check which GPU we have"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GtevwEeaU-wV"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"FrdTblcWHs14"},"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import re\n","import string\n","from collections import Counter, defaultdict\n","from itertools import islice\n","\n","from keras.preprocessing.text import one_hot\n","from keras.layers import Embedding, Dense, Dropout, Flatten\n","from keras import Sequential \n","from keras.preprocessing.sequence import pad_sequences\n","\n","import tensorflow.keras.backend as K\n","import tensorflow as tf\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","from keras.models import model_from_json\n","from pathlib import Path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d765IdmYBdHL"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bD-XQIgUVFd7"},"source":["### Data Processing"]},{"cell_type":"code","metadata":{"id":"Jd7cF09_IGy2"},"source":["DIRECTORY = \"/content/drive/My Drive/NLP A2/\" \n","\n","class Sentences(object):\n","    \"\"\"\n","    Object that allows for reading in of multiple sentences given some filename.\n","    \"\"\"\n","\n","    def __init__(self,filename, vocab = None) -> None:\n","        self.filename = filename\n","        if vocab is None:\n","          self.vocab = self.unk_handling(1)\n","        else: \n","          self.vocab = vocab\n","        self.hash_to_word = defaultdict(lambda:\"<UNK>\")\n","\n","    def unk_handling(self,threshold):\n","      \"\"\"\n","      Returns a set of all vocabulary where the frequency of a word is greater than threshold.\n","      \"\"\"\n","        counter = Counter()\n","        with open(DIRECTORY+self.filename,\"rb\") as file:\n","            for sentence in file:\n","                counter.update(Counter(str(sentence).lower().translate(str.maketrans('','',string.punctuation)).split()))\n","\n","        return {k for k,c in counter.items() if c > threshold}\n","\n","    def __iter__(self):\n","      \"\"\"\n","      Iterator functionality that returns formatted numpy array ready for Keras implementation.\n","      \"\"\"\n","        vocab_length = len(self.vocab)+2\n","        with open(DIRECTORY + self.filename,\"rb\") as file:\n","            for sentence in file:\n","              encoded_arr = [one_hot(\"<s>\",vocab_length)[0]]\n","              for word in [word if word in self.vocab else \"<UNK>\" for word in str(sentence).lower().translate(str.maketrans('','',string.punctuation)).split()]:\n","                  hashed_word = one_hot(word,vocab_length)\n","                  self.hash_to_word[hashed_word[0]] = word\n","                  encoded_arr.append(hashed_word[0])\n","              yield np.array(encoded_arr)\n","\n","def subseqs(seq,window_length):\n","  \"\"\"\n","  Generates a numpy array containing all contigious sequences of some sequence seq where each set os of length window_length.\n","  \"\"\"\n","  return np.fromfunction(lambda i, j: seq[i + j], (len(seq) - window_length + 1, window_length),dtype=int)\n","\n","train_sentences = Sentences(\"nchlt_text.nr.train\")\n","val_sentences = Sentences(\"nchlt_text.nr.valid\", train_sentences.vocab)\n","test_sentences = Sentences(\"nchlt_text.nr.test\", train_sentences.vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7KNTJsZPrGx"},"source":["train = []\n","val = []\n","test = []\n","\n","window_length = 3 # n size for an n-gram model\n","\n","for vec in train_sentences:\n","  train.extend(subseqs(vec,window_length))\n","for vec in val_sentences:\n","  val.extend(subseqs(vec,window_length))\n","for vec in test_sentences:\n","  test.extend(subseqs(vec,window_length))\n","\n","train = pd.DataFrame(train)\n","val = pd.DataFrame(val)\n","test = pd.DataFrame(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7wfhdSRzs_N"},"source":["# Formatting for Keras\n","\n","X_train = np.array(train.iloc[:,0:window_length-1])\n","y_train = np.array(train.iloc[:,window_length-1])\n","\n","X_val = np.array(val.iloc[:,0:window_length-1])\n","y_val = np.array(val.iloc[:,window_length-1])\n","\n","X_test = np.array(test.iloc[:,0:window_length-1])\n","y_test = np.array(test.iloc[:,window_length-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBRgn8A2Sla7"},"source":["X_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tDWqYKAeVMLG"},"source":["### Neural Network Model"]},{"cell_type":"code","metadata":{"id":"y4G2_jZHWzpu"},"source":["vocab_size = len(train_sentences.vocab)+2\n","vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YFvZx--lfwxC"},"source":["# Perplexity metric\n","def perplexity(y_true, y_pred):\n","   \"\"\"\n","    Perplexity = exp(cross entropy loss). Metric for Keras.\n","   \"\"\"\n","   scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","   perplexity = K.exp(scce(y_true, y_pred))\n","   return perplexity\n","\n","# Custom function used for early stopping of model training \n","custom_early_stopping = EarlyStopping(\n","    monitor='val_loss', \n","    patience=2, \n","    min_delta=0.0001 # amount of change to quantify an improvement\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZb7sMFFXspV"},"source":["# define the model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 1000, input_length=window_length-1))\n","model.add(Flatten())\n","model.add(Dense(1024, activation = 'relu'))\n","model.add(Dropout(0.2))\n","model.add(Dense(2048, activation = 'relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(vocab_size, activation='softmax'))\n","\n","# compile the model\n","model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.5),\n","              loss='sparse_categorical_crossentropy', \n","              metrics=['accuracy', perplexity])\n","# summarize the model\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVem8DjPis9O"},"source":["epochs = 50\n","batch_size = 4096"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_47PwOM8iRFc"},"source":["# Train\n","model_history = model.fit(x = X_train, \n","                                y = y_train, \n","                                epochs=epochs,\n","                                batch_size=batch_size,\n","                                validation_data = (X_val,y_val),\n","                                callbacks=[custom_early_stopping],\n","                                verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iidnGiWrVRNU"},"source":["# evaluate the model\n","loss, accuracy, _ = model.evaluate(X_test, y_test, verbose=0)\n","print(f'Loss: {round(loss,4)}')\n","print(f'Perplexity: {round(np.exp(loss),4)}')\n","print(f'Accuracy: {round(accuracy,4)}')"],"execution_count":null,"outputs":[]}]}